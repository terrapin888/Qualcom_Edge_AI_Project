

# #0824 ìˆ˜ì •
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from ultralytics import YOLO
import easyocr
import os
import onnxruntime as ort
import numpy as np

# ---- ì¶”ê°€ ì„í¬íŠ¸ (ì„œë¸Œí”„ë¡œì„¸ìŠ¤/ì„ì‹œíŒŒì¼/ì§ë ¬í™” ìš©) ----
import subprocess
import tempfile
import json
import sys
import textwrap
from pathlib import Path

# ONNX ëª¨ë¸ ì„¤ì •
USE_ONNX = True  # True: ONNX ëª¨ë¸ ì‚¬ìš©, False: Nomic API ì‚¬ìš©
ONNX_MODEL_PATH = "C:/EMpilot/MailPilot_back/models/onnx_model/nomic_embed_text/model.onnx"
EASYOCR_DETECTOR_PATH = "C:/EMpilot/MailPilot_back/models/onnx_model/easyocr-easyocrdetector/model.onnx"
EASYOCR_RECOGNIZER_PATH = "C:/EMpilot/MailPilot_back/models/onnx_model/easyocr-easyocrrecognizer/model.onnx"
YOLO_ONNX_PATH = "C:/EMpilot/MailPilot_back/models/onnx_model/yolo/model.onnx"
USE_EASYOCR_ONNX = True  # True: EasyOCR ONNX ì‚¬ìš©, False: EasyOCR API ì‚¬ìš©
USE_YOLO_ONNX = True  # True: YOLO ONNX ì‚¬ìš©, False: PyTorch YOLO ì‚¬ìš©

# ----- NPU(QNN) ì™¸ë¶€ ì‹¤í–‰ì— í•„ìš”í•œ ê²½ë¡œ(ë„¤ í™˜ê²½ì— ë§ê²Œ ê¸°ë³¸ê°’ ì„¸íŒ…) -----
QNN_DIR_PATH = r"C:\WoS_AI"  # ORT_QNN_Setupì„ í–ˆë˜ ë£¨íŠ¸
QNN_ONNX_MODEL_PATH = r"C:\WoS_AI\Models\nomic\model.onnx\model.onnx"  # NPUì—ì„œ ì“¸ nomic onnx
QNN_SETUP_PS1 = r"C:\WoS_AI\Downloads\Setup_Scripts\ort_setup.ps1"     # Activate_ORT_QNN_VENV ìˆëŠ” ìŠ¤í¬ë¦½íŠ¸

# Nomic API (í´ë°±ìš©)
try:
    from nomic import embed, login
    NOMIC_API_AVAILABLE = True
except ImportError:
    NOMIC_API_AVAILABLE = False

class AIModels:
    def __init__(self, config):
        self.config = config
        
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë“¤
        self.yolo_model = None
        self.qwen_model = None
        self.qwen_tokenizer = None
        self.ocr_reader = None
        self.summarizer = None
        
        # ONNX ëª¨ë¸ ì´ˆê¸°í™”
        self.onnx_session = None
        self.bert_tokenizer = None
        self.easyocr_detector_session = None
        self.easyocr_recognizer_session = None
        self.yolo_onnx_session = None
        
        # NPU ì„¸ì…˜ ì¶”ê°€ (ì™„ì „í•œ NPU íŒŒì´í”„ë¼ì¸)
        self.npu_detector_session = None
        self.npu_recognizer_session = None
        self.npu_yolo_session = None
        
        # Nomic ONNX ëª¨ë¸ (GPU ìš°ì„ , CPU í´ë°±)
        if USE_ONNX and os.path.exists(ONNX_MODEL_PATH):
            self.onnx_session = self._load_onnx_model(ONNX_MODEL_PATH, "Nomic ì„ë² ë”©")
            if self.onnx_session:
                try:
                    # (ì´ë¦„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€) nomic í† í¬ë‚˜ì´ì €ë¡œ êµì²´í•´ ì •í™•ë„ ë§ì¶¤
                    self.bert_tokenizer = AutoTokenizer.from_pretrained("nomic-ai/nomic-embed-text-v1.5", use_fast=True)
                    self._reset_console_color()
                    print("[âœ… ONNX] Nomic ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
                except Exception as e:
                    self._reset_console_color()
                    print(f"[âŒ ONNX] Nomic í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e}")
                    self.onnx_session = None
        
        # EasyOCR ONNX ëª¨ë¸ (GPU ìš°ì„ , CPU í´ë°±)
        if USE_EASYOCR_ONNX:
            if os.path.exists(EASYOCR_DETECTOR_PATH):
                self.easyocr_detector_session = self._load_onnx_model(EASYOCR_DETECTOR_PATH, "EasyOCR Detector")
                if self.easyocr_detector_session:
                    print("[âœ… EasyOCR] Detector ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
                else:
                    print("[âŒ EasyOCR] Detector ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨!")
                self._reset_console_color()
            else:
                print(f"[âŒ EasyOCR] Detector ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {EASYOCR_DETECTOR_PATH}")
            
            if os.path.exists(EASYOCR_RECOGNIZER_PATH):
                self.easyocr_recognizer_session = self._load_onnx_model(EASYOCR_RECOGNIZER_PATH, "EasyOCR Recognizer")
                if self.easyocr_recognizer_session:
                    print("[âœ… EasyOCR] Recognizer ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
                else:
                    print("[âŒ EasyOCR] Recognizer ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨!")
                self._reset_console_color()
            else:
                print(f"[âŒ EasyOCR] Recognizer ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {EASYOCR_RECOGNIZER_PATH}")
        
        # YOLO ONNX ëª¨ë¸ (GPU ìš°ì„ , CPU í´ë°±)
        if USE_YOLO_ONNX and os.path.exists(YOLO_ONNX_PATH):
            try:
                self.yolo_onnx_session = self._load_onnx_model(YOLO_ONNX_PATH, "YOLOv11 ê°ì²´íƒì§€")
            except Exception as e:
                print(f"[âš ï¸ YOLO] GPU ë¡œë”© ì‹¤íŒ¨, CPUë¡œ ì¬ì‹œë„: {e}")
                # CPUë¡œë§Œ ë¡œë“œ
                self.yolo_onnx_session = ort.InferenceSession(
                    YOLO_ONNX_PATH,
                    providers=['CPUExecutionProvider']
                )
                print("[âœ… YOLO] CPU ë¡œë”© ì™„ë£Œ!")
            self._reset_console_color()  # YOLO ë¡œë”© í›„ì—ë„ ìƒ‰ìƒ ë¦¬ì…‹
        
        # NPU ì„¸ì…˜ ë¡œë”© ì‹œë„ (QNN ì§ì ‘ ì‚¬ìš©)
        if USE_EASYOCR_ONNX:
            if os.path.exists(EASYOCR_DETECTOR_PATH) and os.path.exists(EASYOCR_RECOGNIZER_PATH):
                self._try_load_npu_sessions()
        
        # Nomic API ë¡œê·¸ì¸ (í´ë°±ìš©)
        if NOMIC_API_AVAILABLE and not self.onnx_session:
            try:
                login(token=config.NOMIC_TOKEN)
                print("[âœ… Nomic API ë¡œê·¸ì¸ ì™„ë£Œ]")
            except:
                print("[âš ï¸ Nomic API ë¡œê·¸ì¸ ì‹¤íŒ¨]")
    
    def _reset_console_color(self):
        """ì½˜ì†” ìƒ‰ìƒ ë¦¬ì…‹ (ONNX ì—ëŸ¬ í›„ ìƒ‰ìƒ ë³µêµ¬)"""
        import sys
        if sys.platform == "win32":
            import os
            os.system('')  # Windows ì½˜ì†” ANSI í™œì„±í™”
        print('\033[0m', end='')  # ANSI ë¦¬ì…‹ ì½”ë“œ
    
    def _load_onnx_model(self, model_path, model_name):
        """ONNX ëª¨ë¸ ë¡œë”© (GPU ìš°ì„ , CPU í´ë°±)"""
        # ONNX ë¡œê·¸ ë ˆë²¨ì„ WARNINGìœ¼ë¡œ ì„¤ì • (ì—ëŸ¬ ë©”ì‹œì§€ ìˆ¨ê¹€)
        ort.set_default_logger_severity(3)  # 0=VERBOSE, 1=INFO, 2=WARNING, 3=ERROR, 4=FATAL
        
        providers_to_try = [
            ('CUDAExecutionProvider', {'device_id': 0}),  # GPU
            ('CPUExecutionProvider', {})  # CPU
        ]
        
        for provider_name, provider_options in providers_to_try:
            try:
                device_type = "GPU" if "CUDA" in provider_name else "CPU"
                print(f"[ğŸš€ ONNX] {model_name} ëª¨ë¸ ë¡œë”© ì‹œë„ ({device_type})...")
                
                session = ort.InferenceSession(
                    model_path,
                    providers=[provider_name],
                    provider_options=[provider_options]
                )
                
                # ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ í”„ë¡œë°”ì´ë” í™•ì¸
                actual_provider = session.get_providers()[0]
                actual_device = "GPU" if "CUDA" in actual_provider else "CPU"
                
                self._reset_console_color()
                print(f"[âœ… ONNX] {model_name} ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ({actual_device}: {actual_provider})")
                
                return session
                
            except Exception as e:
                # GPU ì‹¤íŒ¨ëŠ” ì¡°ìš©íˆ ì²˜ë¦¬, CPU ì‹¤íŒ¨ë§Œ ë¡œê·¸
                if "CUDA" not in provider_name:
                    print(f"[âŒ ONNX] {model_name} {device_type} ë¡œë”© ì‹¤íŒ¨: {e}")
                continue
        
        print(f"[âŒ ONNX] {model_name} ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ëª¨ë“  í”„ë¡œë°”ì´ë” ì‹œë„í•¨")
        return None
    
    def _try_load_npu_sessions(self):
        """NPU ì„¸ì…˜ ë¡œë”© ì‹œë„ (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì§ì ‘)"""
        try:
            print("[ğŸš€ NPU] ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ NPU ì„¸ì…˜ ë¡œë”© ì‹œë„...")
            
            # QNN ì„¸ì…˜ ì˜µì…˜
            so = ort.SessionOptions()
            so.add_session_config_entry("session.disable_cpu_ep_fallback", "0")
            
            ep_opts = {
                "backend_path": "QnnHtp.dll",
                "htp_performance_mode": "high_performance",
            }
            
            # Detector ì„¸ì…˜ ìƒì„±
            try:
                print("[ğŸš€ NPU] Detector ì„¸ì…˜ ìƒì„± ì¤‘...")
                self.npu_detector_session = ort.InferenceSession(
                    EASYOCR_DETECTOR_PATH,
                    sess_options=so,
                    providers=["QNNExecutionProvider"],
                    provider_options=[ep_opts]
                )
                print("[âœ… NPU] Detector ì„¸ì…˜ ìƒì„± ì™„ë£Œ!")
            except Exception as e:
                print(f"[âŒ NPU] Detector ì„¸ì…˜ ì‹¤íŒ¨: {e}")
                self.npu_detector_session = None
            
            # NPU Recognizer ì„¸ì…˜ ìƒì„±
            try:
                print("[ğŸš€ NPU] Recognizer ì„¸ì…˜ ìƒì„± ì¤‘...")
                self.npu_recognizer_session = ort.InferenceSession(
                    EASYOCR_RECOGNIZER_PATH,
                    sess_options=so,
                    providers=["QNNExecutionProvider"],
                    provider_options=[ep_opts]
                )
                print("[âœ… NPU] Recognizer ì„¸ì…˜ ìƒì„± ì™„ë£Œ!")
            except Exception as e:
                print(f"[âŒ NPU] Recognizer ì„¸ì…˜ ì‹¤íŒ¨: {e}")
                self.npu_recognizer_session = None
            
            # YOLO NPU ì„¸ì…˜ ìƒì„± (ì„ íƒì‚¬í•­)
            if os.path.exists(YOLO_ONNX_PATH):
                try:
                    print("[ğŸš€ NPU] YOLO ì„¸ì…˜ ìƒì„± ì¤‘...")
                    self.npu_yolo_session = ort.InferenceSession(
                        YOLO_ONNX_PATH,
                        sess_options=so,
                        providers=["QNNExecutionProvider"],
                        provider_options=[ep_opts]
                    )
                    print("[âœ… NPU] YOLO ì„¸ì…˜ ìƒì„± ì™„ë£Œ!")
                except Exception as e:
                    print(f"[âŒ NPU] YOLO ì„¸ì…˜ ì‹¤íŒ¨: {e}")
                    self.npu_yolo_session = None
            
            if self.npu_detector_session and self.npu_recognizer_session:
                print("[ğŸ‰ NPU] ì™„ì „í•œ NPU íŒŒì´í”„ë¼ì¸ ì„¸ì…˜ ì¤€ë¹„ ì™„ë£Œ!")
            else:
                print("[âš ï¸ NPU] ì¼ë¶€ NPU ì„¸ì…˜ ë¡œë”© ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"[âŒ NPU] ë©”ì¸ í”„ë¡œì„¸ìŠ¤ NPU ë¡œë”© ì‹¤íŒ¨: {e}")
            # NPU ì„¸ì…˜ ëª¨ë‘ Noneìœ¼ë¡œ ì„¤ì •
            self.npu_detector_session = None
            self.npu_recognizer_session = None
            self.npu_yolo_session = None
    
    def load_yolo_model(self):
        """YOLO ëª¨ë¸ ë¡œë”©"""
        if self.yolo_model is None:
            try:
                print("[ğŸ¤– YOLOv8 ëª¨ë¸ ë¡œë”© ì‹œì‘]")
                self.yolo_model = YOLO(self.config.YOLO_MODEL)
                print("[âœ… YOLOv8 ëª¨ë¸ ë¡œë”© ì™„ë£Œ]")
                return True
            except Exception as e:
                print(f"[â—YOLO ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨] {str(e)}")
                return False
        return True
    
    def load_qwen_model(self):
        """Qwen ëª¨ë¸ ë¡œë”© (GPU ìš°ì„ , CPU í´ë°±)"""
        if self.qwen_model is None:
            print("[ğŸ¤– Qwen ëª¨ë¸ ë¡œë”© ì‹œì‘]")
            
            # í† í¬ë‚˜ì´ì € ë¡œë”©
            try:
                self.qwen_tokenizer = AutoTokenizer.from_pretrained(
                    self.config.QWEN_MODEL, 
                    trust_remote_code=True
                )
                print("[âœ… Qwen í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ]")
            except Exception as e:
                print(f"[âŒ Qwen í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨] {str(e)}")
                return False
            
            # ëª¨ë¸ ë¡œë”© (GPU ìš°ì„  ì‹œë„)
            if torch.cuda.is_available():
                try:
                    print("[ğŸš€ Qwen] GPU ë¡œë”© ì‹œë„...")
                    self.qwen_model = AutoModelForCausalLM.from_pretrained(
                        self.config.QWEN_MODEL,
                        torch_dtype=torch.float16,
                        trust_remote_code=True,
                        device_map=None
                    )
                    self.qwen_model.to("cuda")
                    self.qwen_model.eval()
                    print("[âœ… Qwen] GPU ë¡œë”© ì™„ë£Œ!")
                    return True
                except Exception as e:
                    print(f"[âš ï¸ Qwen] GPU ë¡œë”© ì‹¤íŒ¨, CPUë¡œ í´ë°±: {str(e)}")
            
            # CPU í´ë°±
            try:
                print("[ğŸš€ Qwen] CPU ë¡œë”© ì‹œë„...")
                self.qwen_model = AutoModelForCausalLM.from_pretrained(
                    self.config.QWEN_MODEL,
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                self.qwen_model.to("cpu")
                self.qwen_model.eval()
                print("[âœ… Qwen] CPU ë¡œë”© ì™„ë£Œ!")
                return True
            except Exception as e:
                print(f"[âŒ Qwen] CPU ë¡œë”©ë„ ì‹¤íŒ¨: {str(e)}")
                return False
        return True
    
    def load_ocr_model(self):
        """OCR ëª¨ë¸ ë¡œë”© (ONNX ìš°ì„ , EasyOCR API í´ë°±)"""
        # ONNX ëª¨ë¸ì´ ì´ë¯¸ ë¡œë”©ë˜ì–´ ìˆìœ¼ë©´
        if self.easyocr_detector_session and self.easyocr_recognizer_session:
            return True
            
        # EasyOCR API í´ë°±
        if self.ocr_reader is None:
            try:
                print("[ğŸ“– EasyOCR API ëª¨ë¸ ë¡œë”© ì‹œì‘ (í´ë°±)]")
                self.ocr_reader = easyocr.Reader(['ko', 'en'])
                print("[âœ… EasyOCR API ëª¨ë¸ ë¡œë”© ì™„ë£Œ]")
                return True
            except Exception as e:
                print(f"[â—EasyOCR API ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨] {str(e)}")
                return False
        return True
    
    def _preprocess_image_for_onnx(self, image_np):
        """ONNX OCRì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        import cv2
        
        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ONNX ëª¨ë¸ ìš”êµ¬ì‚¬í•­: 608x800)
        target_height, target_width = 608, 800
        
        # ì´ë¯¸ì§€ë¥¼ ì •í™•í•œ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        resized = cv2.resize(image_np, (target_width, target_height))
        print(f"[ğŸ”§ ONNX OCR] ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ: {image_np.shape[:2]} â†’ {resized.shape[:2]}")
        
        # ì •ê·œí™” (0-255 -> 0-1)
        normalized = resized.astype(np.float32) / 255.0
        
        # CHW í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (Height, Width, Channel -> Channel, Height, Width)
        transposed = np.transpose(normalized, (2, 0, 1))
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (1, C, H, W)
        batched = np.expand_dims(transposed, axis=0)
        
        print(f"[ğŸ”§ ONNX OCR] ìµœì¢… ì…ë ¥ í˜•íƒœ: {batched.shape}")
        return batched
    
    def _postprocess_ocr_result(self, onnx_output, original_image_shape):
        """ONNX OCR ê²°ê³¼ í›„ì²˜ë¦¬"""
        detections = []
        
        try:
            # ONNX ì¶œë ¥: results [1, 304, 400, 2], features [1, 32, 304, 400]
            results = onnx_output[0]  # [1, 304, 400, 2]
            features = onnx_output[1]  # [1, 32, 304, 400]
            
            print(f"[ğŸ”§ ONNX OCR] ê²°ê³¼ í˜•íƒœ: {results.shape}, íŠ¹ì§• í˜•íƒœ: {features.shape}")
            
            # í…ìŠ¤íŠ¸ ì˜ì—­ íƒì§€ (ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜)
            # resultsì˜ ë§ˆì§€ë§‰ ì°¨ì›ì´ [score, class] ë˜ëŠ” [x, y] ì¢Œí‘œì¼ ê°€ëŠ¥ì„±
            batch_results = results[0]  # [304, 400, 2]
            
            # ì„ê³„ê°’ ì´ìƒì˜ ì˜ì—­ ì°¾ê¸°
            threshold = 0.5
            text_regions = []
            
            for i in range(batch_results.shape[0]):
                for j in range(batch_results.shape[1]):
                    score = batch_results[i, j, 0]  # ì²« ë²ˆì§¸ ê°’ì„ ì‹ ë¢°ë„ë¡œ ê°€ì •
                    if score > threshold:
                        # ì¢Œí‘œ ê³„ì‚° (ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§)
                        orig_h, orig_w = original_image_shape[:2]
                        x = j * orig_w / 400  # 400ì€ ëª¨ë¸ ì¶œë ¥ ë„ˆë¹„
                        y = i * orig_h / 304  # 304ëŠ” ëª¨ë¸ ì¶œë ¥ ë†’ì´
                        
                        # EasyOCR í˜•ì‹ìœ¼ë¡œ ë³€í™˜: [ì¢Œí‘œ, í…ìŠ¤íŠ¸, ì‹ ë¢°ë„]
                        detection = [
                            [[x, y], [x+20, y], [x+20, y+10], [x, y+10]],  # ë°”ìš´ë”© ë°•ìŠ¤
                            f"Text_{len(text_regions)}",  # ë”ë¯¸ í…ìŠ¤íŠ¸ (ì‹¤ì œ OCR í•„ìš”)
                            float(score)
                        ]
                        text_regions.append(detection)
            
            print(f"[âœ… ONNX OCR] {len(text_regions)}ê°œ í…ìŠ¤íŠ¸ ì˜ì—­ íƒì§€ë¨")
            return text_regions[:10]  # ìµœëŒ€ 10ê°œë§Œ ë°˜í™˜
            
        except Exception as e:
            print(f"[âŒ ONNX OCR] í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []
    
    def extract_text_from_image_onnx(self, image_np):
        """EasyOCR ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (NPU ìš°ì„ , ë‚®ì€ ì„ê³„ê°’)"""
        print(f"[ğŸ” DEBUG] Detector ì„¸ì…˜: {self.easyocr_detector_session is not None}")
        print(f"[ğŸ” DEBUG] Recognizer ì„¸ì…˜: {self.easyocr_recognizer_session is not None}")
        
        # 1. ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì™„ì „í•œ NPU íŒŒì´í”„ë¼ì¸ ìš°ì„  ì‹œë„
        if self.npu_detector_session and self.npu_recognizer_session:
            try:
                print("[ğŸš€ NPU Direct] ì™„ì „í•œ NPU íŒŒì´í”„ë¼ì¸ ì‹¤í–‰...")
                result = self._process_with_npu_direct(image_np)
                if result is not None:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                    print("[âœ… NPU Direct] ì„±ê³µ")
                    return result
            except Exception as e:
                print(f"[âš ï¸ NPU Direct] ì‹¤íŒ¨, ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ì‹œë„: {e}")
        
        # 2. ì„œë¸Œí”„ë¡œì„¸ìŠ¤ NPU ì‹œë„ (í´ë°±)
        try:
            print("[ğŸš€ NPU Subprocess] ì„œë¸Œí”„ë¡œì„¸ìŠ¤ NPU ì‹¤í–‰...")
            result = self._run_npu_easyocr_via_subprocess(image_np)
            if result is not None:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                print("[âœ… NPU Subprocess] ì„±ê³µ")
                return result
        except Exception as e:
            print(f"[âš ï¸ NPU Subprocess] ì‹¤íŒ¨, ONNXë¡œ í´ë°±: {e}")
        
        # 2. ONNX í´ë°±
        if not self.easyocr_detector_session or not self.easyocr_recognizer_session:
            print("[âŒ OCR DEBUG] ONNX ëª¨ë¸ ì„¸ì…˜ì´ Noneì…ë‹ˆë‹¤")
            return None
        
        print("[ğŸš€ EasyOCR ONNX] í´ë°± ì²˜ë¦¬ ì‹œì‘...")
        try:
            return self._process_with_simple_pipeline(image_np)
        except Exception as e:
            print(f"[âŒ EasyOCR ONNX] ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±: {e}")
            return self._process_with_manual_method(image_np)
    
    def load_summarizer(self):
        """ìš”ì•½ ëª¨ë¸ ë¡œë”© (ë¹„í™œì„±í™” - Qwen ì‚¬ìš©)"""
        print("[â„¹ï¸ BART ìš”ì•½ ëª¨ë¸ ë¹„í™œì„±í™”ë¨ - Qwen ì‚¬ìš©]")
        return False  # í•­ìƒ False ë°˜í™˜í•˜ì—¬ Qwen ì‚¬ìš© ê°•ì œ
    
    # ---------------- NPU ì„ ì‹œë„: ì™¸ë¶€ QNN venvì—ì„œ ì„ë² ë”© ìƒì„± ----------------
    def _run_npu_embed_via_subprocess(self, texts):
        """
        QNN ì „ìš© venvë¥¼ PowerShellì—ì„œ í™œì„±í™”í•œ ë’¤,
        ë³„ë„ ì„ì‹œ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•´ NPU(HTP)ë¡œ ì„ë² ë”©ì„ ìƒì„±.
        ì„±ê³µí•˜ë©´ float32 ndarray ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜, ì‹¤íŒ¨í•˜ë©´ None.
        """
        try:
            # ì„ì‹œ íŒŒì¼ ì¤€ë¹„
            with tempfile.TemporaryDirectory() as td:
                td_path = Path(td)
                in_json = td_path / "input.json"
                out_npy = td_path / "embeddings.npy"
                script_py = td_path / "npu_embed_runner.py"

                # ì…ë ¥ ë°ì´í„° ì €ì¥
                payload = {
                    "texts": texts,
                    "onnx_model_path": QNN_ONNX_MODEL_PATH
                }
                in_json.write_text(json.dumps(payload, ensure_ascii=False), encoding="utf-8")

                # ì„ì‹œ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± (ë„¤ê°€ ì¤€ NPU ì½”ë“œ + I/O)
                script_code = textwrap.dedent(rf"""
                    import json, sys, numpy as np, onnxruntime as ort, time
                    from transformers import AutoTokenizer

                    in_json = r"{in_json}"
                    out_npy = r"{out_npy}"

                    data = json.loads(open(in_json, "r", encoding="utf-8").read())
                    texts = data["texts"]
                    onnx_model_path = data["onnx_model_path"]

                    so = ort.SessionOptions()
                    so.add_session_config_entry("session.disable_cpu_ep_fallback", "1")

                    ep_opts = {{
                        "backend_path": "QnnHtp.dll",
                        "htp_performance_mode": "high_performance",
                    }}

                    session = ort.InferenceSession(
                        onnx_model_path,
                        sess_options=so,
                        providers=["QNNExecutionProvider"],
                        provider_options=[ep_opts],
                    )

                    in_names = [i.name for i in session.get_inputs()]
                    out_name = session.get_outputs()[0].name

                    def pick(cands, target):
                        for nm in cands:
                            low = nm.lower()
                            if target=="tokens" and "token" in low: return nm
                            if target=="mask" and "mask" in low: return nm
                        return None

                    name_tokens = "input_tokens" if "input_tokens" in in_names else pick(in_names, "tokens")
                    name_mask   = "attention_masks" if "attention_masks" in in_names else pick(in_names, "mask")
                    assert name_tokens and name_mask, f"ì…ë ¥ ì´ë¦„ ë§¤í•‘ ì‹¤íŒ¨: {{in_names}}"

                    tokenizer = AutoTokenizer.from_pretrained("nomic-ai/nomic-embed-text-v1.5", use_fast=True)
                    MAX_LEN = 128

                    def enc_one(t):
                        e = tokenizer(t, padding="max_length", truncation=True, max_length=MAX_LEN, return_tensors="np")
                        return {{
                            name_tokens: e["input_ids"].astype(np.int32),
                            name_mask:   e["attention_mask"].astype(np.float32),
                        }}

                    # ì›Œë°ì—…
                    if len(texts) > 0:
                        wf = enc_one(texts[0])
                        for _ in range(2): _ = session.run([out_name], wf)

                    embs = []
                    for t in texts:
                        out = session.run([out_name], enc_one(t))[0]  # (1, D)
                        embs.append(out[0].astype(np.float32))        # raw embedding (ì •ê·œí™” ì—†ìŒ)

                    arr = np.stack(embs, axis=0)  # (N, D)
                    np.save(out_npy, arr)
                """).strip()
                script_py.write_text(script_code, encoding="utf-8")

                # PowerShell ëª…ë ¹ êµ¬ì„±: QNN venv í™œì„±í™” â†’ ì›Œí‚¹ ë””ë ‰í† ë¦¬ ì´ë™ â†’ íŒŒì´ì¬ ì‹¤í–‰
                ps_cmd = (
                    f"& {{"
                    f"  cd '{QNN_DIR_PATH}'; "
                    f"  . '{QNN_SETUP_PS1}'; "
                    f"  Activate_ORT_QNN_VENV -rootDirPath '{QNN_DIR_PATH}'; "
                    f"  Set-Location '{os.getcwd()}'; "  # ë°±ì—”ë“œ ì›Œí‚¹ë””ë ‰í† ë¦¬ ìœ ì§€
                    f"  $env:PYTHONUTF8='1'; "
                    f"  python '{script_py}'; "
                    f"}}"
                )

                # ì‹¤í–‰
                result = subprocess.run(
                    ["powershell", "-NoProfile", "-ExecutionPolicy", "Bypass", "-Command", ps_cmd],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    timeout=300
                )

                # ë¡œê·¸ ì¶œë ¥(ì°¸ê³ )
                if result.stdout:
                    print("[â„¹ï¸ NPU stdout]", result.stdout.strip())
                if result.stderr:
                    print("[â„¹ï¸ NPU stderr]", result.stderr.strip())

                # ê²°ê³¼ í™•ì¸
                if result.returncode == 0 and out_npy.exists():
                    embs = np.load(out_npy).astype(np.float32)
                    # list[np.ndarray]ë¡œ ë³€í™˜ (ê¸°ì¡´ ë°˜í™˜ í˜•ì‹ê³¼ í˜¸í™˜)
                    return [embs[i] for i in range(embs.shape[0])]
                else:
                    print(f"[âš ï¸ NPU] ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨(returncode={result.returncode})")
                    return None

        except Exception as e:
            print(f"[âš ï¸ NPU] ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None

    def _get_embeddings(self, texts):
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (ONNX ìš°ì„ , API í´ë°±)
        - ë¨¼ì € NPU(QNN) ì™¸ë¶€ ì‹¤í–‰ì„ ì‹œë„
        - ì‹¤íŒ¨í•˜ë©´ ê¸°ì¡´ ê²½ë¡œ(í˜„ì¬ venvì˜ ONNX ì„¸ì…˜ â†’ Nomic API)ë¡œ í´ë°±
        """
        # 1) NPU(QNN) ì™¸ë¶€ ì‹¤í–‰ ì„ ì‹œë„
        try:
            print("[ğŸš€ NPU] ì™¸ë¶€ QNN venvì—ì„œ ì„ë² ë”© ì‹œë„...")
            npu_embs = self._run_npu_embed_via_subprocess(texts)
            if npu_embs and len(npu_embs) == len(texts):
                print("[âœ… NPU] ì„ë² ë”© ìƒì„± ì„±ê³µ (QNN/HTP)")
                return {'embeddings': npu_embs}
            else:
                print("[â„¹ï¸ NPU] ì„ë² ë”© ë¯¸ìƒì„± ë˜ëŠ” ê°œìˆ˜ ë¶ˆì¼ì¹˜ â†’ í´ë°± ì§„í–‰")
        except Exception as e:
            print(f"[âš ï¸ NPU] ì˜ˆì™¸ ë°œìƒ â†’ í´ë°±: {e}")

        # 2) í˜„ì¬ venvì˜ ONNX ì„¸ì…˜ ì‚¬ìš© (GPU â†’ CPU)
        if self.onnx_session and self.bert_tokenizer:
            # ONNX ëª¨ë¸ ì‚¬ìš©
            try:
                print(f"[ğŸš€ ONNX] ì„ë² ë”© ìƒì„± ì‹œì‘ - {len(texts)}ê°œ í…ìŠ¤íŠ¸")
                embeddings = []
                for i, text in enumerate(texts):
                    inputs = self.bert_tokenizer(
                        text, 
                        padding="max_length", 
                        max_length=128, 
                        truncation=True,
                        return_tensors="np"
                    )
                    
                    outputs = self.onnx_session.run(None, {
                        "input_tokens": inputs["input_ids"].astype(np.int32),
                        "attention_masks": inputs["attention_mask"].astype(np.float32)
                    })
                    embeddings.append(outputs[0][0])
                    print(f"[âœ… ONNX] í…ìŠ¤íŠ¸ {i+1}/{len(texts)} ì„ë² ë”© ì™„ë£Œ (ì°¨ì›: {len(outputs[0][0])})")
                
                print(f"[ğŸ‰ ONNX] ì „ì²´ ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
                return {'embeddings': embeddings}
            except Exception as e:
                print(f"[âš ï¸ ONNX] ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        
        # 3) Nomic API ì‚¬ìš© (í´ë°±)
        if NOMIC_API_AVAILABLE:
            return embed.text(texts, model='nomic-embed-text-v1', task_type='classification')
        else:
            raise Exception("ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def classify_email(self, text):
        """ì´ë©”ì¼ ë¶„ë¥˜"""
        try:
            text_inputs = [text] + self.config.CANDIDATE_LABELS
            result = self._get_embeddings(text_inputs)
            
            embedding_list = result['embeddings']
            email_embedding = [embedding_list[0]]
            label_embeddings = embedding_list[1:]
            
            from sklearn.metrics.pairwise import cosine_similarity
            scores = cosine_similarity(email_embedding, label_embeddings)[0]
            best_index = scores.argmax()
            
            return {
                'classification': self.config.CANDIDATE_LABELS[best_index],
                'confidence': scores[best_index]
            }
            
        except Exception as e:
            print(f"[âš ï¸ ë¶„ë¥˜ ì‹¤íŒ¨] {str(e)}")
            return {'classification': 'unknown', 'confidence': 0.0}
    
    def _run_npu_easyocr_via_subprocess(self, image_np):
        """
        QNN venvë¥¼ PowerShellì—ì„œ í™œì„±í™”í•œ ë’¤,
        ë³„ë„ ì„ì‹œ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•´ NPU(HTP)ë¡œ EasyOCRì„ ì‹¤í–‰.
        ì„±ê³µí•˜ë©´ í…ìŠ¤íŠ¸ ì˜ì—­ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜, ì‹¤íŒ¨í•˜ë©´ None.
        """
        try:
            import tempfile, json, os
            from pathlib import Path
            
            with tempfile.TemporaryDirectory() as td:
                td_path = Path(td)
                in_json = td_path / "easyocr_input.json"
                out_json = td_path / "easyocr_output.json"
                script_py = td_path / "npu_easyocr_runner.py"
                img_path = td_path / "input_image.npy"
                
                # ì´ë¯¸ì§€ ì €ì¥
                np.save(img_path, image_np)
                
                # EasyOCR Reader ë¬¸ìì…‹ ê°€ì ¸ì˜¤ê¸°
                if not hasattr(self, '_easyocr_reader'):
                    import easyocr
                    self._easyocr_reader = easyocr.Reader(['en'], gpu=False)
                    print(f"[ğŸ” EasyOCR] Reader ì´ˆê¸°í™” ì™„ë£Œ - ë¬¸ìì…‹: {len(self._easyocr_reader.character)}ê°œ")
                
                # ì…ë ¥ ë°ì´í„° ì €ì¥ (ë¬¸ìì…‹ í¬í•¨)
                payload = {
                    "image_path": str(img_path),
                    "detector_path": EASYOCR_DETECTOR_PATH,
                    "recognizer_path": EASYOCR_RECOGNIZER_PATH,
                    "charset": list(self._easyocr_reader.character)  # ì‹¤ì œ EasyOCR ë¬¸ìì…‹ ì „ë‹¬
                }
                in_json.write_text(json.dumps(payload, ensure_ascii=False), encoding="utf-8")
                
                # NPU EasyOCR ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
                script_code = textwrap.dedent(rf"""
                    import json, sys, numpy as np, onnxruntime as ort, cv2
                    from pathlib import Path
                    
                    in_json = r"{in_json}"
                    out_json = r"{out_json}"
                    
                    try:
                        # ì…ë ¥ ë¡œë“œ
                        data = json.loads(open(in_json, "r", encoding="utf-8").read())
                        image_np = np.load(data["image_path"])
                        detector_path = data["detector_path"]
                        recognizer_path = data["recognizer_path"]
                        
                        # QNN ì„¸ì…˜ ì˜µì…˜
                        so = ort.SessionOptions()
                        so.add_session_config_entry("session.disable_cpu_ep_fallback", "0")
                        
                        ep_opts = {{
                            "backend_path": "QnnHtp.dll",
                            "htp_performance_mode": "high_performance",
                        }}
                        
                        # Detector ì„¸ì…˜
                        detector_session = ort.InferenceSession(
                            detector_path,
                            sess_options=so,
                            providers=["QNNExecutionProvider"],
                            provider_options=[ep_opts]
                        )
                        
                        # Recognizer ì„¸ì…˜
                        recognizer_session = ort.InferenceSession(
                            recognizer_path,
                            sess_options=so,
                            providers=["QNNExecutionProvider"],
                            provider_options=[ep_opts]
                        )
                        
                        print("[NPU EasyOCR] ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                        
                        # EasyOCR ì „ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ë³µì‚¬)
                        from easyocr.craft_utils import getDetBoxes, adjustResultCoordinates
                        from easyocr.imgproc import normalizeMeanVariance
                        
                        target_height, target_width = 608, 800
                        resized = cv2.resize(image_np, (target_width, target_height))
                        normalized = normalizeMeanVariance(resized)
                        transposed = np.transpose(normalized, (2, 0, 1))
                        batched = np.expand_dims(transposed, axis=0).astype(np.float32)
                        
                        # Detector ì‹¤í–‰
                        detector_output = detector_session.run(None, {{"image": batched}})
                        results = detector_output[0]
                        
                        # Detector í›„ì²˜ë¦¬
                        score_text = results[0][:, :, 0]
                        score_link = results[0][:, :, 1]
                        
                        print(f"[ğŸ” NPU DEBUG] ìŠ¤ì½”ì–´ ë²”ìœ„: text={{score_text.min():.4f}}~{{score_text.max():.4f}}, link={{score_link.min():.4f}}~{{score_link.max():.4f}}")
                        print(f"[ğŸ” NPU DEBUG] ìŠ¤ì½”ì–´ í˜•íƒœ: text={{score_text.shape}}, link={{score_link.shape}}")
                        
                        # 0.01 ì´ìƒì¸ í”½ì…€ ê°œìˆ˜ í™•ì¸
                        high_text_count = np.sum(score_text > 0.01)
                        high_link_count = np.sum(score_link > 0.01) 
                        print(f"[ğŸ” NPU DEBUG] 0.01 ì´ìƒ í”½ì…€: text={{high_text_count}}, link={{high_link_count}}")
                        
                        boxes, polys, mapper = getDetBoxes(
                            score_text, score_link,
                            text_threshold=0.2, link_threshold=0.15, low_text=0.15, poly=False
                        )
                        
                        print(f"[ğŸ” NPU DEBUG] getDetBoxes ê²°ê³¼: {{len(boxes)}}ê°œ ë°•ìŠ¤")
                        if len(boxes) > 0:
                            print(f"[ğŸ” NPU DEBUG] ì²« ë²ˆì§¸ ë°•ìŠ¤: {{boxes[0] if boxes[0] is not None else 'None'}}")
                        
                        if len(boxes) == 0:
                            with open(out_json, "w", encoding="utf-8") as f:
                                json.dump({{"success": True, "text_regions": []}}, f)
                            sys.exit(0)
                        
                        # ì¢Œí‘œ ì¡°ì • (ë°±ì—… íŒŒì¼ ë°©ì‹)
                        orig_h, orig_w = image_np.shape[:2]
                        ratio_w = orig_w / target_width
                        ratio_h = orig_h / target_height
                        boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)
                        
                        # ê° ë°•ìŠ¤ì—ì„œ í…ìŠ¤íŠ¸ ì¸ì‹ (ì‹¤ì œ Recognizer ì‚¬ìš©)
                        text_regions = []
                        for i, box in enumerate(boxes):  # ëª¨ë“  ë°•ìŠ¤ ì²˜ë¦¬
                            if box is None or len(box) != 4:
                                continue
                                
                            # ë°•ìŠ¤ ì¢Œí‘œ ì¶”ì¶œ
                            xs = [p[0] for p in box]
                            ys = [p[1] for p in box]
                            x1, x2 = int(min(xs)), int(max(xs))
                            y1, y2 = int(min(ys)), int(max(ys))
                            
                            if (x2 - x1) < 5 or (y2 - y1) < 5:
                                continue
                            
                            # ì´ë¯¸ì§€ í¬ë¡­
                            cropped = image_np[y1:y2, x1:x2]
                            if cropped.size == 0:
                                continue
                            
                            # Recognizerë¡œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì¸ì‹
                            try:
                                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
                                if len(cropped.shape) == 3:
                                    gray_image = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)
                                else:
                                    gray_image = cropped
                                
                                # PIL Imageë¡œ ë³€í™˜
                                from PIL import Image
                                pil_image = Image.fromarray(gray_image).convert('L')
                                
                                # Recognizer ì…ë ¥ í¬ê¸° (64x1000)
                                imgH, imgW = 64, 1000
                                
                                # ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ
                                h, w = pil_image.size[1], pil_image.size[0]  # PILì€ (w, h)
                                ratio = min(imgW / w, imgH / h)
                                new_w = int(w * ratio)
                                new_h = int(h * ratio)
                                
                                resized = pil_image.resize((new_w, new_h), Image.LANCZOS)
                                
                                # íŒ¨ë”© ì¶”ê°€ (ì¤‘ì•™ ì •ë ¬)
                                padded = Image.new('L', (imgW, imgH), color=0)
                                paste_x = (imgW - new_w) // 2
                                paste_y = (imgH - new_h) // 2
                                padded.paste(resized, (paste_x, paste_y))
                                
                                # NumPy ë°°ì—´ë¡œ ë³€í™˜ ë° ì •ê·œí™”
                                img_array = np.array(padded).astype(np.float32) / 255.0
                                
                                # ë°°ì¹˜ ë° ì±„ë„ ì°¨ì› ì¶”ê°€ (1, 1, 64, 1000)
                                input_tensor = np.expand_dims(np.expand_dims(img_array, 0), 0)
                                
                                # Recognizer ì‹¤í–‰
                                recognizer_outputs = recognizer_session.run(None, {{"image": input_tensor}})
                                
                                # ì›ë³¸ ai_models.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë””ì½”ë”©
                                try:
                                    # EasyOCR Reader ì´ˆê¸°í™” (converter ê°€ì ¸ì˜¤ê¸° ìœ„í•´)
                                    import easyocr
                                    import torch
                                    import torch.nn.functional as F
                                    
                                    temp_reader = easyocr.Reader(['en'], gpu=False)
                                    
                                    # ì¶œë ¥ì„ PyTorch Tensorë¡œ ë³€í™˜
                                    output = recognizer_outputs[0]
                                    preds = torch.from_numpy(output).float()
                                    
                                    print(f"[ğŸ” NPU CTC] ì…ë ¥ í˜•íƒœ: {{preds.shape}}")
                                    
                                    # Softmax ì ìš© (ì›ë³¸ê³¼ ë™ì¼)
                                    preds = F.softmax(preds, dim=2)
                                    
                                    # Greedy ë””ì½”ë”© (ì›ë³¸ê³¼ ë™ì¼)
                                    _, preds_index = preds.max(2)
                                    
                                    # ê¸¸ì´ ì •ë³´ ìƒì„±
                                    batch_size = preds.size(0)
                                    preds_size = torch.IntTensor([preds.size(1)] * batch_size)
                                    
                                    # ì‹¤ì œ EasyOCR Converterë¡œ ë””ì½”ë”© (ì›ë³¸ê³¼ ë™ì¼)
                                    converter = temp_reader.converter
                                    preds_str = converter.decode_greedy(
                                        preds_index.view(-1).data.cpu().detach().numpy(), 
                                        preds_size.data
                                    )
                                    
                                    recognized_text = preds_str[0] if preds_str else ""
                                    print(f"[âœ… NPU CTC] ì›ë³¸ ë°©ì‹ ë””ì½”ë”©: '{{recognized_text}}'")
                                    
                                except Exception as converter_e:
                                    print(f"[âš ï¸ NPU CTC] EasyOCR converter ì‹¤íŒ¨, ê°„ì†Œí™” ë°©ì‹ ì‚¬ìš©: {{converter_e}}")
                                    # í´ë°±: ê°„ì†Œí™”ëœ CTC ë””ì½”ë”©
                                    output_probs = recognizer_outputs[0][0]
                                    predicted_ids = np.argmax(output_probs, axis=1)
                                    charset = data.get("charset", [])
                                    if len(charset) > 1:
                                        decoded_chars = []
                                        prev_char = -1
                                        for char_id in predicted_ids:
                                            if char_id != prev_char and char_id > 0 and char_id < len(charset):
                                                decoded_chars.append(charset[char_id])
                                            prev_char = char_id
                                        recognized_text = ''.join(decoded_chars).strip()
                                    else:
                                        recognized_text = ""
                                
                                if recognized_text:
                                    text_regions.append([
                                        [[x1, y1], [x2, y1], [x2, y2], [x1, y2]],
                                        recognized_text,
                                        0.8
                                    ])
                                    print(f"[âœ… NPU OCR] Box {{i+1}}: '{{recognized_text}}'")
                                else:
                                    # ë¹ˆ í…ìŠ¤íŠ¸ì¸ ê²½ìš° ë”ë¯¸ë¡œ ëŒ€ì²´
                                    text_regions.append([
                                        [[x1, y1], [x2, y1], [x2, y2], [x1, y2]],
                                        f"NPU_Text_{{i+1}}",
                                        0.5
                                    ])
                                    
                            except Exception as recog_e:
                                print(f"[âš ï¸ NPU OCR] Recognizer ì‹¤íŒ¨ (Box {{i+1}}): {{recog_e}}")
                                # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ í…ìŠ¤íŠ¸
                                text_regions.append([
                                    [[x1, y1], [x2, y1], [x2, y2], [x1, y2]],
                                    f"NPU_Text_{{i+1}}",
                                    0.3
                                ])
                        
                        # ê²°ê³¼ ì €ì¥
                        with open(out_json, "w", encoding="utf-8") as f:
                            json.dump({{"success": True, "text_regions": text_regions}}, f)
                        
                    except Exception as e:
                        with open(out_json, "w", encoding="utf-8") as f:
                            json.dump({{"success": False, "error": str(e)}}, f)
                """).strip()
                
                script_py.write_text(script_code, encoding="utf-8")
                
                # PowerShell ëª…ë ¹ (Nomicê³¼ ë™ì¼í•œ êµ¬ì¡°)
                ps_cmd = (
                    f"& {{"
                    f"  cd '{QNN_DIR_PATH}'; "
                    f"  . '{QNN_SETUP_PS1}'; "
                    f"  Activate_ORT_QNN_VENV -rootDirPath '{QNN_DIR_PATH}'; "
                    f"  Set-Location '{os.getcwd()}'; "
                    f"  $env:PYTHONUTF8='1'; "
                    f"  python '{script_py}'; "
                    f"}}"
                )
                
                # ì‹¤í–‰
                result = subprocess.run(
                    ["powershell", "-NoProfile", "-ExecutionPolicy", "Bypass", "-Command", ps_cmd],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    timeout=300
                )
                
                # ê²°ê³¼ í™•ì¸
                if result.returncode == 0 and out_json.exists():
                    output_data = json.loads(out_json.read_text(encoding="utf-8"))
                    if output_data.get("success"):
                        print(f"[âœ… NPU EasyOCR] {len(output_data['text_regions'])}ê°œ ì˜ì—­ íƒì§€ë¨")
                        return output_data["text_regions"]
                    else:
                        print(f"[âŒ NPU EasyOCR] ìŠ¤í¬ë¦½íŠ¸ ì˜¤ë¥˜: {output_data.get('error')}")
                else:
                    print(f"[âš ï¸ NPU EasyOCR] í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨(returncode={result.returncode})")
                    if result.stderr:
                        print(f"[stderr] {result.stderr}")
                
                return None
                
        except Exception as e:
            print(f"[âš ï¸ NPU EasyOCR] ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None
    
    def _run_npu_yolo_via_subprocess(self, image_np):
        """
        QNN venvë¥¼ PowerShellì—ì„œ í™œì„±í™”í•œ ë’¤,
        ë³„ë„ ì„ì‹œ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•´ NPU(HTP)ë¡œ YOLOë¥¼ ì‹¤í–‰.
        ì„±ê³µí•˜ë©´ íƒì§€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜, ì‹¤íŒ¨í•˜ë©´ None.
        """
        try:
            import tempfile, json, os
            from pathlib import Path
            
            with tempfile.TemporaryDirectory() as td:
                td_path = Path(td)
                in_json = td_path / "yolo_input.json"
                out_json = td_path / "yolo_output.json"
                script_py = td_path / "npu_yolo_runner.py"
                img_path = td_path / "input_image.npy"
                
                # ì´ë¯¸ì§€ ì €ì¥
                np.save(img_path, image_np)
                
                # ì…ë ¥ ë°ì´í„° ì €ì¥
                payload = {
                    "image_path": str(img_path),
                    "yolo_path": YOLO_ONNX_PATH
                }
                in_json.write_text(json.dumps(payload, ensure_ascii=False), encoding="utf-8")
                
                # NPU YOLO ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
                script_code = textwrap.dedent(rf"""
                    import json, sys, numpy as np, onnxruntime as ort, cv2
                    from pathlib import Path
                    
                    in_json = r"{in_json}"
                    out_json = r"{out_json}"
                    
                    try:
                        # ì…ë ¥ ë¡œë“œ
                        data = json.loads(open(in_json, "r", encoding="utf-8").read())
                        image_np = np.load(data["image_path"])
                        yolo_path = data["yolo_path"]
                        
                        # QNN ì„¸ì…˜ ì˜µì…˜
                        so = ort.SessionOptions()
                        so.add_session_config_entry("session.disable_cpu_ep_fallback", "0")
                        
                        ep_opts = {{
                            "backend_path": "QnnHtp.dll",
                            "htp_performance_mode": "high_performance",
                        }}
                        
                        # YOLO ì„¸ì…˜
                        yolo_session = ort.InferenceSession(
                            yolo_path,
                            sess_options=so,
                            providers=["QNNExecutionProvider"],
                            provider_options=[ep_opts]
                        )
                        
                        print("[NPU YOLO] ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                        
                        # YOLO ì „ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ë³µì‚¬)
                        input_size = 640
                        original_height, original_width = image_np.shape[:2]
                        
                        # ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ ë¦¬ì‚¬ì´ì¦ˆ
                        scale = min(input_size / original_width, input_size / original_height)
                        new_width = int(original_width * scale)
                        new_height = int(original_height * scale)
                        
                        # ë¦¬ì‚¬ì´ì¦ˆ
                        resized = cv2.resize(image_np, (new_width, new_height))
                        
                        # íŒ¨ë”© ì¶”ê°€ (ì¤‘ì•™ ì •ë ¬)
                        pad_x = (input_size - new_width) // 2
                        pad_y = (input_size - new_height) // 2
                        
                        padded = np.full((input_size, input_size, 3), 114, dtype=np.uint8)
                        padded[pad_y:pad_y+new_height, pad_x:pad_x+new_width] = resized
                        
                        # ì°¨ì› ë³€ê²½ (HWC -> CHW) - uint8 ìœ ì§€
                        transposed = np.transpose(padded, (2, 0, 1))
                        batched = np.expand_dims(transposed, axis=0)
                        
                        # YOLO ì‹¤í–‰
                        input_name = yolo_session.get_inputs()[0].name
                        outputs = yolo_session.run(None, {{input_name: batched}})
                        
                        # YOLO í›„ì²˜ë¦¬ (ONNXì™€ ì™„ì „íˆ ë™ì¼í•˜ê²Œ)
                        predictions = outputs[0][0]  # (1, 84, 8400) -> (84, 8400)
                        
                        # ì¢Œí‘œì™€ ì‹ ë¢°ë„ ë¶„ë¦¬
                        boxes = predictions[:4]  # x, y, w, h
                        scores = predictions[4:]  # í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ (80, 8400)
                        
                        # ìµœëŒ€ ì‹ ë¢°ë„ì™€ í´ë˜ìŠ¤ ID
                        class_scores = np.max(scores, axis=0)  # (8400,)
                        class_ids = np.argmax(scores, axis=0)  # (8400,)
                        
                        # ì‹ ë¢°ë„ ì„ê³„ê°’ ì ìš©
                        conf_threshold = 0.5
                        valid_indices = class_scores > conf_threshold
                        
                        if not np.any(valid_indices):
                            with open(out_json, "w", encoding="utf-8") as f:
                                json.dump({{"success": True, "detections": []}}, f)
                            sys.exit(0)
                        
                        valid_boxes = boxes[:, valid_indices]  # (4, N)
                        valid_scores = class_scores[valid_indices]  # (N,)
                        valid_class_ids = class_ids[valid_indices]  # (N,)
                        
                        # ì¢Œí‘œ ë³€í™˜ (ì¤‘ì‹¬ì , ë„ˆë¹„, ë†’ì´ -> x1, y1, x2, y2)
                        x_center = valid_boxes[0]
                        y_center = valid_boxes[1]
                        width = valid_boxes[2]
                        height = valid_boxes[3]
                        
                        x1 = x_center - width / 2
                        y1 = y_center - height / 2
                        x2 = x_center + width / 2
                        y2 = y_center + height / 2
                        
                        # ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ì—­ë³€í™˜
                        x1 = (x1 - pad_x) / scale
                        y1 = (y1 - pad_y) / scale
                        x2 = (x2 - pad_x) / scale
                        y2 = (y2 - pad_y) / scale
                        
                        # ì¢Œí‘œ í´ë¦¬í•‘
                        x1 = np.clip(x1, 0, original_width)
                        y1 = np.clip(y1, 0, original_height)
                        x2 = np.clip(x2, 0, original_width)
                        y2 = np.clip(y2, 0, original_height)
                        
                        # ê°„ë‹¨í•œ NMS (OpenCV ì—†ì´)
                        keep_indices = []
                        order = valid_scores.argsort()[::-1]
                        
                        while len(order) > 0:
                            i = order[0]
                            keep_indices.append(i)
                            
                            if len(order) == 1:
                                break
                            
                            # IoU ê³„ì‚°
                            box1 = [x1[i], y1[i], x2[i], y2[i]]
                            remaining_boxes = np.column_stack([x1[order[1:]], y1[order[1:]], x2[order[1:]], y2[order[1:]]])
                            
                            # ê°„ë‹¨í•œ IoU
                            x1_max = np.maximum(box1[0], remaining_boxes[:, 0])
                            y1_max = np.maximum(box1[1], remaining_boxes[:, 1])
                            x2_min = np.minimum(box1[2], remaining_boxes[:, 2])
                            y2_min = np.minimum(box1[3], remaining_boxes[:, 3])
                            
                            intersection_area = np.maximum(0, x2_min - x1_max) * np.maximum(0, y2_min - y1_max)
                            
                            box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
                            boxes_area = (remaining_boxes[:, 2] - remaining_boxes[:, 0]) * (remaining_boxes[:, 3] - remaining_boxes[:, 1])
                            
                            union_area = box1_area + boxes_area - intersection_area
                            ious = intersection_area / (union_area + 1e-6)
                            
                            keep = np.where(ious <= 0.45)[0]
                            order = order[keep + 1]
                        
                        # COCO í´ë˜ìŠ¤ ì´ë¦„
                        class_names = [
                            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
                            'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',
                            'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',
                            'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
                            'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
                            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
                            'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
                            'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',
                            'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
                            'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
                        ]
                        
                        # ê²°ê³¼ êµ¬ì„±
                        detections = []
                        for i in keep_indices[:10]:  # ìµœëŒ€ 10ê°œë§Œ ì²˜ë¦¬
                            class_id = int(valid_class_ids[i])
                            confidence = float(valid_scores[i])
                            class_name = class_names[class_id] if class_id < len(class_names) else f"class_{{class_id}}"
                            
                            detections.append({{
                                'class': class_name,
                                'confidence': confidence,
                                'class_id': class_id,
                                'bbox': [float(x1[i]), float(y1[i]), float(x2[i]), float(y2[i])]
                            }})
                        
                        # ê²°ê³¼ ì €ì¥
                        with open(out_json, "w", encoding="utf-8") as f:
                            json.dump({{"success": True, "detections": detections}}, f)
                        
                    except Exception as e:
                        with open(out_json, "w", encoding="utf-8") as f:
                            json.dump({{"success": False, "error": str(e)}}, f)
                """).strip()
                
                script_py.write_text(script_code, encoding="utf-8")
                
                # PowerShell ëª…ë ¹ (Nomicê³¼ ë™ì¼í•œ êµ¬ì¡°)
                ps_cmd = (
                    f"& {{"
                    f"  cd '{QNN_DIR_PATH}'; "
                    f"  . '{QNN_SETUP_PS1}'; "
                    f"  Activate_ORT_QNN_VENV -rootDirPath '{QNN_DIR_PATH}'; "
                    f"  Set-Location '{os.getcwd()}'; "
                    f"  $env:PYTHONUTF8='1'; "
                    f"  python '{script_py}'; "
                    f"}}"
                )
                
                # ì‹¤í–‰
                result = subprocess.run(
                    ["powershell", "-NoProfile", "-ExecutionPolicy", "Bypass", "-Command", ps_cmd],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    timeout=300
                )
                
                # ê²°ê³¼ í™•ì¸
                if result.returncode == 0 and out_json.exists():
                    output_data = json.loads(out_json.read_text(encoding="utf-8"))
                    if output_data.get("success"):
                        detections = output_data["detections"]
                        print(f"[âœ… NPU YOLO] {len(detections)}ê°œ ê°ì²´ íƒì§€ë¨")
                        # ê°œë³„ íƒì§€ ê²°ê³¼ ì¶œë ¥
                        for detection in detections:
                            class_name = detection.get('class', 'unknown')
                            confidence = detection.get('confidence', 0.0)
                            print(f"[âœ… NPU YOLO] íƒì§€: {class_name} ({confidence:.3f})")
                        return detections
                    else:
                        print(f"[âŒ NPU YOLO] ìŠ¤í¬ë¦½íŠ¸ ì˜¤ë¥˜: {output_data.get('error')}")
                else:
                    print(f"[âš ï¸ NPU YOLO] í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨(returncode={result.returncode})")
                    if result.stderr:
                        print(f"[stderr] {result.stderr}")
                
                return None
                
        except Exception as e:
            print(f"[âš ï¸ NPU YOLO] ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None
    
    def detect_objects_with_yolo_onnx(self, image_np):
        """YOLO ONNX ëª¨ë¸ë¡œ ê°ì²´ íƒì§€ (NPU ìš°ì„ )"""
        # 1. NPU ìš°ì„  ì‹œë„ (Nomic ë°©ì‹)
        try:
            print("[ğŸš€ NPU YOLO] NPU ì‹¤í–‰ ì‹œë„...")
            result = self._run_npu_yolo_via_subprocess(image_np)
            if result:
                print("[âœ… NPU YOLO] ì„±ê³µ")
                return result
        except Exception as e:
            print(f"[âš ï¸ NPU YOLO] ì‹¤íŒ¨, ONNXë¡œ í´ë°±: {e}")
        
        # 2. ONNX ì„¸ì…˜ í´ë°±
        if not self.yolo_onnx_session:
            print("[âŒ YOLO ONNX] ì„¸ì…˜ì´ Noneì…ë‹ˆë‹¤")
            return []
            
        try:
            import cv2
            
            print("[ğŸš€ YOLO ONNX] ê°ì²´ íƒì§€ ì‹œì‘...")
            
            # YOLOv11 ì…ë ¥ í¬ê¸° (640x640)
            input_size = 640
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            original_height, original_width = image_np.shape[:2]
            
            # ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ ë¦¬ì‚¬ì´ì¦ˆ
            scale = min(input_size / original_width, input_size / original_height)
            new_width = int(original_width * scale)
            new_height = int(original_height * scale)
            
            # ë¦¬ì‚¬ì´ì¦ˆ
            resized = cv2.resize(image_np, (new_width, new_height))
            
            # íŒ¨ë”© ì¶”ê°€ (ì¤‘ì•™ ì •ë ¬)
            pad_x = (input_size - new_width) // 2
            pad_y = (input_size - new_height) // 2
            
            padded = np.full((input_size, input_size, 3), 114, dtype=np.uint8)  # Gray padding
            padded[pad_y:pad_y+new_height, pad_x:pad_x+new_width] = resized
            
            # ì°¨ì› ë³€ê²½ (HWC -> CHW) - uint8 ìœ ì§€
            transposed = np.transpose(padded, (2, 0, 1))
            batched = np.expand_dims(transposed, axis=0)
            
            print(f"[ğŸ”§ YOLO] ì „ì²˜ë¦¬ ì™„ë£Œ: {image_np.shape} -> {batched.shape}")
            
            # ONNX ì¶”ë¡ 
            input_name = self.yolo_onnx_session.get_inputs()[0].name
            outputs = self.yolo_onnx_session.run(None, {input_name: batched})
            
            # YOLOv11 ì¶œë ¥ í›„ì²˜ë¦¬
            predictions = outputs[0][0]  # (1, 84, 8400) -> (84, 8400)
            
            print(f"[ğŸ”§ YOLO] ì¶œë ¥ í˜•íƒœ: {predictions.shape}")
            
            # ì¢Œí‘œì™€ ì‹ ë¢°ë„ ë¶„ë¦¬
            boxes = predictions[:4]  # x, y, w, h
            scores = predictions[4:]  # í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„
            
            # ìµœëŒ€ ì‹ ë¢°ë„ì™€ í´ë˜ìŠ¤ ID
            class_scores = np.max(scores, axis=0)
            class_ids = np.argmax(scores, axis=0)
            
            # ì‹ ë¢°ë„ ì„ê³„ê°’ ì ìš©
            conf_threshold = 0.5
            valid_indices = class_scores > conf_threshold
            
            if not np.any(valid_indices):
                print("[âŒ YOLO] ì‹ ë¢°ë„ ì„ê³„ê°’ì„ ë„˜ëŠ” ê°ì²´ ì—†ìŒ")
                return []
            
            valid_boxes = boxes[:, valid_indices]
            valid_scores = class_scores[valid_indices]
            valid_class_ids = class_ids[valid_indices]
            
            # ì¢Œí‘œ ë³€í™˜ (ì¤‘ì‹¬ì , ë„ˆë¹„, ë†’ì´ -> x1, y1, x2, y2)
            x_center = valid_boxes[0]
            y_center = valid_boxes[1]
            width = valid_boxes[2]
            height = valid_boxes[3]
            
            x1 = x_center - width / 2
            y1 = y_center - height / 2
            x2 = x_center + width / 2
            y2 = y_center + height / 2
            
            # ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ì—­ë³€í™˜
            x1 = (x1 - pad_x) / scale
            y1 = (y1 - pad_y) / scale
            x2 = (x2 - pad_x) / scale
            y2 = (y2 - pad_y) / scale
            
            # ì¢Œí‘œ í´ë¦¬í•‘
            x1 = np.clip(x1, 0, original_width)
            y1 = np.clip(y1, 0, original_height)
            x2 = np.clip(x2, 0, original_width)
            y2 = np.clip(y2, 0, original_height)
            
            # NMS ì ìš©
            boxes_for_nms = np.column_stack([x1, y1, x2, y2])
            keep_indices = self._apply_nms(boxes_for_nms, valid_scores, iou_threshold=0.45)
            
            # COCO í´ë˜ìŠ¤ ì´ë¦„ ì •ì˜ (YOLOv11 ê¸°ë³¸)
            class_names = [
                'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
                'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',
                'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',
                'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
                'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
                'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
                'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
                'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',
                'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
                'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
            ]
            
            # ê²°ê³¼ êµ¬ì„±
            detections = []
            for i in keep_indices:
                class_id = int(valid_class_ids[i])
                confidence = float(valid_scores[i])
                class_name = class_names[class_id] if class_id < len(class_names) else f"class_{class_id}"
                
                detections.append({
                    'class': class_name,
                    'confidence': confidence,
                    'class_id': class_id,
                    'bbox': [float(x1[i]), float(y1[i]), float(x2[i]), float(y2[i])]
                })
                
                print(f"[âœ… YOLO] íƒì§€: {class_name} ({confidence:.3f})")
            
            print(f"[âœ… YOLO ONNX] {len(detections)}ê°œ ê°ì²´ íƒì§€ë¨")
            return detections
            
        except Exception as e:
            print(f"[âŒ YOLO ONNX] ì¶”ë¡  ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _apply_nms(self, boxes, scores, iou_threshold=0.45):
        """Non-Maximum Suppression ì ìš©"""
        try:
            import cv2
            
            # OpenCV NMS ì‚¬ìš©
            indices = cv2.dnn.NMSBoxes(
                boxes.tolist(), 
                scores.tolist(), 
                score_threshold=0.1, 
                nms_threshold=iou_threshold
            )
            
            if len(indices) > 0:
                return indices.flatten()
            else:
                return []
                
        except Exception as e:
            print(f"[âš ï¸ NMS] OpenCV NMS ì‹¤íŒ¨, ìˆ˜ë™ êµ¬í˜„ ì‚¬ìš©: {e}")
            
            # ìˆ˜ë™ NMS êµ¬í˜„
            indices = []
            order = scores.argsort()[::-1]  # ì‹ ë¢°ë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            
            while len(order) > 0:
                i = order[0]
                indices.append(i)
                
                if len(order) == 1:
                    break
                
                # IoU ê³„ì‚°
                ious = self._calculate_iou(boxes[i], boxes[order[1:]])
                
                # IoU ì„ê³„ê°’ ì´í•˜ì¸ ë°•ìŠ¤ë“¤ë§Œ ìœ ì§€
                keep = np.where(ious <= iou_threshold)[0]
                order = order[keep + 1]
            
            return indices
    
    def _calculate_iou(self, box1, boxes):
        """IoU (Intersection over Union) ê³„ì‚°"""
        x1_max = np.maximum(box1[0], boxes[:, 0])
        y1_max = np.maximum(box1[1], boxes[:, 1])
        x2_min = np.minimum(box1[2], boxes[:, 2])
        y2_min = np.minimum(box1[3], boxes[:, 3])
        
        intersection_area = np.maximum(0, x2_min - x1_max) * np.maximum(0, y2_min - y1_max)
        
        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
        boxes_area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
        
        union_area = box1_area + boxes_area - intersection_area
        
        return intersection_area / (union_area + 1e-6)
    
    def _process_with_npu_direct(self, image_np):
        """ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ NPU ì§ì ‘ ì‹¤í–‰ (ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ì—†ìŒ)"""
        import cv2
        from easyocr.craft_utils import getDetBoxes, adjustResultCoordinates
        from easyocr.imgproc import normalizeMeanVariance
        
        print("[ğŸš€ NPU Direct] NPU ì§ì ‘ ì²˜ë¦¬ ì‹œì‘...")
        
        try:
            # 1. Detector ì „ì²˜ë¦¬
            target_height, target_width = 608, 800
            resized = cv2.resize(image_np, (target_width, target_height))
            normalized = normalizeMeanVariance(resized)
            transposed = np.transpose(normalized, (2, 0, 1))
            batched = np.expand_dims(transposed, axis=0).astype(np.float32)
            
            # 2. NPU Detector ì‹¤í–‰
            detector_output = self.npu_detector_session.run(None, {"image": batched})
            results = detector_output[0]
            
            # 3. Detector í›„ì²˜ë¦¬
            score_text = results[0][:, :, 0]
            score_link = results[0][:, :, 1]
            
            print(f"[ğŸ”§ NPU] ìŠ¤ì½”ì–´ ë²”ìœ„: text={score_text.min():.4f}~{score_text.max():.4f}")
            
            boxes, polys, mapper = getDetBoxes(
                score_text, score_link,
                text_threshold=0.2, link_threshold=0.15, low_text=0.15, poly=False
            )
            
            print(f"[ğŸ”§ NPU] íƒì§€ëœ ë°•ìŠ¤: {len(boxes)}ê°œ")
            
            if len(boxes) == 0:
                print("[âŒ NPU] í…ìŠ¤íŠ¸ ì˜ì—­ì´ íƒì§€ë˜ì§€ ì•ŠìŒ")
                return []
            
            # ì¢Œí‘œ ì¡°ì •
            orig_h, orig_w = image_np.shape[:2]
            ratio_w = orig_w / target_width
            ratio_h = orig_h / target_height
            boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)
            
            # 4. ê° ë°•ìŠ¤ì—ì„œ í…ìŠ¤íŠ¸ ì¸ì‹
            text_regions = []
            for i, box in enumerate(boxes):
                if box is None or len(box) != 4:
                    continue
                    
                # ë°•ìŠ¤ ì¢Œí‘œ ì¶”ì¶œ
                xs = [p[0] for p in box]
                ys = [p[1] for p in box]
                x1, x2 = int(min(xs)), int(max(xs))
                y1, y2 = int(min(ys)), int(max(ys))
                
                if (x2 - x1) < 5 or (y2 - y1) < 5:
                    continue
                
                # ì´ë¯¸ì§€ í¬ë¡­
                cropped = image_np[y1:y2, x1:x2]
                if cropped.size == 0:
                    continue
                
                # NPU Recognizerë¡œ í…ìŠ¤íŠ¸ ì¸ì‹ (ì™„ì „í•œ NPU íŒŒì´í”„ë¼ì¸)
                recognized_text = self._recognize_text_with_npu(cropped)
                
                if recognized_text and recognized_text != "???":
                    detection = [
                        [[x1, y1], [x2, y1], [x2, y2], [x1, y2]],
                        recognized_text,
                        0.8
                    ]
                    text_regions.append(detection)
                    print(f"[âœ… NPU] ì™„ì „í•œ NPU íŒŒì´í”„ë¼ì¸ Box {i+1}: '{recognized_text}'")
            
            print(f"[âœ… NPU Direct] ì™„ì „í•œ NPU íŒŒì´í”„ë¼ì¸ - {len(text_regions)}ê°œ í…ìŠ¤íŠ¸ ì˜ì—­ ì¸ì‹ë¨")
            return text_regions
            
        except Exception as e:
            print(f"[âŒ NPU Direct] ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    
    def _recognize_text_with_npu(self, cropped_image):
        """NPU Recognizerë¡œ í…ìŠ¤íŠ¸ ì¸ì‹ (PyTorch converter ì‚¬ìš©)"""
        try:
            import cv2
            from easyocr.recognition import AlignCollate
            from torch.utils.data import DataLoader, Dataset
            import torch
            
            # ListDataset êµ¬í˜„
            class SimpleListDataset(Dataset):
                def __init__(self, img_list):
                    self.img_list = img_list
                def __len__(self):
                    return len(self.img_list)
                def __getitem__(self, idx):
                    return self.img_list[idx]
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(cropped_image.shape) == 3:
                gray_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)
            else:
                gray_image = cropped_image
            
            # PIL Imageë¡œ ë³€í™˜
            from PIL import Image
            pil_image = Image.fromarray(gray_image).convert('L')
            
            # ì „ì²˜ë¦¬ (EasyOCR ë°©ì‹)
            imgH, imgW = 64, 1000
            AlignCollate_normal = AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True)
            
            img_list = [pil_image]
            test_data = SimpleListDataset(img_list)
            test_loader = DataLoader(test_data, batch_size=1, shuffle=False, num_workers=0, 
                                   collate_fn=AlignCollate_normal, pin_memory=False)
            
            # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
            for batch in test_loader:
                if isinstance(batch, torch.Tensor):
                    normalized = batch.numpy()
                else:
                    normalized = batch[0].numpy() if isinstance(batch[0], torch.Tensor) else batch[0]
                
                # NPU Recognizer ì‹¤í–‰
                outputs = self.npu_recognizer_session.run(None, {"image": normalized})
                
                # PyTorch EasyOCR ë°©ì‹ìœ¼ë¡œ ë””ì½”ë”© (ì •í™•ë„ ìš°ì„ )
                print(f"[ğŸ”¥ NPU] Recognizer ì‹¤í–‰ ì™„ë£Œ")
                return self._decode_with_easyocr_converter(outputs[0])
            
            return ""
            
        except Exception as e:
            print(f"[âš ï¸ NPU Recognizer] ì‹¤íŒ¨: {e}")
            return "???"
    
    def _recognize_text_with_onnx(self, cropped_image):
        """ONNX CPU Recognizerë¡œ í…ìŠ¤íŠ¸ ì¸ì‹ (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)"""
        try:
            import cv2
            from easyocr.recognition import AlignCollate
            from torch.utils.data import DataLoader, Dataset
            import torch
            
            # ListDataset êµ¬í˜„
            class SimpleListDataset(Dataset):
                def __init__(self, img_list):
                    self.img_list = img_list
                def __len__(self):
                    return len(self.img_list)
                def __getitem__(self, idx):
                    return self.img_list[idx]
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(cropped_image.shape) == 3:
                gray_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)
            else:
                gray_image = cropped_image
            
            # PIL Imageë¡œ ë³€í™˜
            from PIL import Image
            pil_image = Image.fromarray(gray_image).convert('L')
            
            # ì „ì²˜ë¦¬ (EasyOCR ë°©ì‹)
            imgH, imgW = 64, 1000
            AlignCollate_normal = AlignCollate(imgH=imgH, imgW=imgW, keep_ratio_with_pad=True)
            
            img_list = [pil_image]
            test_data = SimpleListDataset(img_list)
            test_loader = DataLoader(test_data, batch_size=1, shuffle=False, num_workers=0, 
                                   collate_fn=AlignCollate_normal, pin_memory=False)
            
            # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
            for batch in test_loader:
                if isinstance(batch, torch.Tensor):
                    normalized = batch.numpy()
                else:
                    normalized = batch[0].numpy() if isinstance(batch[0], torch.Tensor) else batch[0]
                
                # ONNX CPU Recognizer ì‹¤í–‰
                outputs = self.easyocr_recognizer_session.run(None, {"image": normalized})
                
                # PyTorch EasyOCR ë°©ì‹ìœ¼ë¡œ ë””ì½”ë”© (ì •í™•ë„ ìš°ì„ )
                print(f"[ğŸ”¥ ONNX CPU] Recognizer ì‹¤í–‰ ì™„ë£Œ")
                return self._decode_with_easyocr_converter(outputs[0])
            
            return ""
            
        except Exception as e:
            print(f"[âš ï¸ ONNX Recognizer] ì‹¤íŒ¨: {e}")
            return "???"
    
    
    def _process_with_simple_pipeline(self, image_np):
        """ë°”íƒ•í™”ë©´ í…ŒìŠ¤íŠ¸ì—ì„œ ê²€ì¦ëœ ê°„ë‹¨í•œ íŒŒì´í”„ë¼ì¸"""
        import cv2
        from easyocr.craft_utils import getDetBoxes, adjustResultCoordinates
        from easyocr.imgproc import normalizeMeanVariance
        
        print("[ğŸš€ Simple Pipeline] ê°„ë‹¨í•œ OCR íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
        
        try:
            # 1. Detector ì „ì²˜ë¦¬
            print("[ğŸ”§] Detector ì „ì²˜ë¦¬...")
            target_height, target_width = 608, 800
            resized = cv2.resize(image_np, (target_width, target_height))
            normalized = normalizeMeanVariance(resized)
            transposed = np.transpose(normalized, (2, 0, 1))
            batched = np.expand_dims(transposed, axis=0).astype(np.float32)
            
            # 2. Detector ì‹¤í–‰
            print("[ğŸ”§] Detector ì‹¤í–‰...")
            detector_output = self.easyocr_detector_session.run(None, {"image": batched})
            results = detector_output[0]  # [1, 304, 400, 2]
            
            # 3. Detector í›„ì²˜ë¦¬
            print("[ğŸ”§] Detector í›„ì²˜ë¦¬...")
            score_text = results[0][:, :, 0]
            score_link = results[0][:, :, 1]
            
            print(f"[ğŸ”§] ìŠ¤ì½”ì–´ ë²”ìœ„: text={score_text.min():.4f}~{score_text.max():.4f}")
            
            # ë‚®ì€ ì„ê³„ê°’ ì‚¬ìš© (í…ŒìŠ¤íŠ¸ì—ì„œ ê²€ì¦ë¨)
            boxes, polys, mapper = getDetBoxes(
                score_text, score_link,
                text_threshold=0.2, link_threshold=0.15, low_text=0.15, poly=False
            )
            
            print(f"[ğŸ”§] íƒì§€ëœ ë°•ìŠ¤: {len(boxes)}ê°œ")
            
            if len(boxes) == 0:
                print("[âŒ] í…ìŠ¤íŠ¸ ì˜ì—­ì´ íƒì§€ë˜ì§€ ì•ŠìŒ")
                return []
            
            # ì¢Œí‘œ ì¡°ì •
            orig_h, orig_w = image_np.shape[:2]
            ratio_w = orig_w / target_width
            ratio_h = orig_h / target_height
            
            boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)
            
            # 4. ê° ë°•ìŠ¤ì—ì„œ í…ìŠ¤íŠ¸ ì¸ì‹
            text_regions = []
            for i, box in enumerate(boxes):
                if box is None or len(box) != 4:
                    continue
                    
                # ë°•ìŠ¤ ì¢Œí‘œ ì¶”ì¶œ
                xs = [p[0] for p in box]
                ys = [p[1] for p in box]
                x1, x2 = int(min(xs)), int(max(xs))
                y1, y2 = int(min(ys)), int(max(ys))
                
                # ìœ íš¨í•œ í¬ê¸°ì¸ì§€ í™•ì¸
                if (x2 - x1) < 5 or (y2 - y1) < 5:
                    continue
                
                # ì´ë¯¸ì§€ í¬ë¡­
                cropped = image_np[y1:y2, x1:x2]
                
                # Recognizerë¡œ í…ìŠ¤íŠ¸ ì¸ì‹
                recognized_text = self._recognize_text_from_crop(cropped)
                
                if recognized_text and recognized_text != "???":
                    detection = [
                        [[x1, y1], [x2, y1], [x2, y2], [x1, y2]],
                        recognized_text,
                        0.8
                    ]
                    text_regions.append(detection)
                    print(f"[âœ…] Box {i+1}: '{recognized_text}'")
            
            print(f"[âœ… Simple Pipeline] {len(text_regions)}ê°œ í…ìŠ¤íŠ¸ ì˜ì—­ ì¸ì‹ë¨")
            return text_regions
            
        except Exception as e:
            print(f"[âŒ Simple Pipeline] ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _process_with_manual_method(self, image_np):
        """ìˆ˜ë™ ë°©ì‹ OCR ì²˜ë¦¬"""
        try:
            import cv2
            
            print("[ğŸš€ ONNX OCR] ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì¤‘...")
            preprocessed = self._preprocess_image_for_onnx(image_np)
            
            print("[ğŸš€ ONNX OCR] í…ìŠ¤íŠ¸ íƒì§€ ì‹¤í–‰ ì¤‘...")
            # Detector ì‹¤í–‰
            detector_outputs = self.easyocr_detector_session.run(None, {"image": preprocessed})
            
            print("[ğŸš€ ONNX OCR] í…ìŠ¤íŠ¸ ì˜ì—­ ì¶”ì¶œ ì¤‘...")
            # íƒì§€ëœ ì˜ì—­ ì¶”ì¶œ
            text_regions = self._postprocess_ocr_result(detector_outputs, image_np.shape)
            
            print(f"[ğŸš€ ONNX OCR] {len(text_regions)}ê°œ ì˜ì—­ì—ì„œ í…ìŠ¤íŠ¸ ì¸ì‹ ì¤‘...")
            # Recognizerë¡œ ê° ì˜ì—­ì˜ í…ìŠ¤íŠ¸ ì¸ì‹
            recognized_results = []
            
            for i, region in enumerate(text_regions):
                bbox, dummy_text, confidence = region
                
                # bboxì—ì„œ ì¢Œí‘œ ì¶”ì¶œ (4ê°œ ì )
                points = np.array(bbox, dtype=np.int32)
                
                # íŒ¨ë”©ì„ ì¶”ê°€í•˜ì—¬ ë” í° ì˜ì—­ ì¶”ì¶œ (í…ìŠ¤íŠ¸ ì£¼ë³€ ì—¬ë°± í¬í•¨)
                padding = 10  # ê° ë°©í–¥ìœ¼ë¡œ 10í”½ì…€ íŒ¨ë”©
                x_min = max(0, np.min(points[:, 0]) - padding)
                y_min = max(0, np.min(points[:, 1]) - padding)
                x_max = min(image_np.shape[1], np.max(points[:, 0]) + padding)
                y_max = min(image_np.shape[0], np.max(points[:, 1]) + padding)
                
                print(f"[ğŸ”§ OCR] ì˜ì—­ {i+1} í¬ë¡­: ({x_min},{y_min}) - ({x_max},{y_max}) = {x_max-x_min}x{y_max-y_min}")
                
                # í…ìŠ¤íŠ¸ ì˜ì—­ crop
                cropped = image_np[y_min:y_max, x_min:x_max]
                
                if cropped.size == 0:
                    continue
                
                # Recognizerë¥¼ ìœ„í•œ ì „ì²˜ë¦¬
                recognized_text = self._recognize_text_from_crop(cropped)
                
                if recognized_text:
                    # ì‹¤ì œ í…ìŠ¤íŠ¸ë¡œ êµì²´
                    recognized_results.append([bbox, recognized_text, confidence])
                    print(f"[âœ… OCR] ì˜ì—­ {i+1}: {recognized_text}")
            
            return recognized_results
            
        except Exception as e:
            print(f"[âŒ ONNX OCR] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _recognize_text_from_crop(self, cropped_image):
        """ì‹¤ì œ EasyOCR ë°©ì‹ìœ¼ë¡œ cropped ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¸ì‹"""
        try:
            import cv2
            from easyocr.recognition import AlignCollate
            from torch.utils.data import DataLoader, Dataset
            import torch
            import torch.nn.functional as F
            
            # ListDatasetì„ ì§ì ‘ êµ¬í˜„
            class SimpleListDataset(Dataset):
                def __init__(self, img_list):
                    self.img_list = img_list
                
                def __len__(self):
                    return len(self.img_list)
                
                def __getitem__(self, idx):
                    return self.img_list[idx]
            
            # EasyOCR Reader ì´ˆê¸°í™” (ë¬¸ìì…‹ê³¼ converter ê°€ì ¸ì˜¤ê¸°)
            if not hasattr(self, '_easyocr_reader'):
                import easyocr
                self._easyocr_reader = easyocr.Reader(['en'], gpu=False)
                print(f"[ğŸ” EasyOCR] Reader ì´ˆê¸°í™” ì™„ë£Œ - ë¬¸ìì…‹: {len(self._easyocr_reader.character)}ê°œ")
            
            # ì´ë¯¸ì§€ë¥¼ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            if len(cropped_image.shape) == 3:
                gray_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)
            else:
                gray_image = cropped_image
            
            # PIL Imageë¡œ ë³€í™˜ (AlignCollateê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹)
            from PIL import Image
            pil_image = Image.fromarray(gray_image).convert('L')  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼
            
            # ONNX ëª¨ë¸ ê³ ì • í¬ê¸° (1000x64)
            imgH = 64
            imgW = 1000  # ONNX ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ê³ ì • ë„ˆë¹„
            h, w = gray_image.shape
            
            print(f"[ğŸ” EasyOCR] ê³ ì • í¬ê¸°: {imgW}x{imgH} (ì›ë³¸: {w}x{h})")
            
            # AlignCollateë¡œ ì „ì²˜ë¦¬
            AlignCollate_normal = AlignCollate(
                imgH=imgH, 
                imgW=imgW, 
                keep_ratio_with_pad=True
            )
            
            # PIL Image ë¦¬ìŠ¤íŠ¸ë¡œ ì¤€ë¹„
            img_list = [pil_image]
            test_data = SimpleListDataset(img_list)
            test_loader = DataLoader(
                test_data,
                batch_size=1,
                shuffle=False,
                num_workers=0,
                collate_fn=AlignCollate_normal,
                pin_memory=False,
            )
            
            # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
            for batch in test_loader:
                # PyTorch Tensorë¥¼ NumPyë¡œ ë³€í™˜
                if isinstance(batch, torch.Tensor):
                    normalized = batch.numpy()
                else:
                    # batchê°€ tuple/listì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
                    normalized = batch[0].numpy() if isinstance(batch[0], torch.Tensor) else batch[0]
                
                print(f"[ğŸ” ì‹¤ì œ ì…ë ¥] í˜•íƒœ: {normalized.shape}, íƒ€ì…: {normalized.dtype}")
                
                # Recognizer ì‹¤í–‰
                outputs = self.easyocr_recognizer_session.run(None, {"image": normalized})
                
                # ì‹¤ì œ EasyOCR ë°©ì‹ìœ¼ë¡œ ë””ì½”ë”©
                recognized_text = self._decode_with_easyocr_converter(outputs[0])
                
                return recognized_text
            
            return ""
            
        except Exception as e:
            print(f"[âš ï¸ Recognizer] EasyOCR ë°©ì‹ ì¸ì‹ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def _decode_with_easyocr_converter(self, output):
        """ì‹¤ì œ EasyOCR Converterë¡œ ë””ì½”ë”© (í…ŒìŠ¤íŠ¸ì—ì„œ ê²€ì¦ëœ ë°©ì‹)"""
        try:
            import torch
            import torch.nn.functional as F
            
            # EasyOCR Readerì—ì„œ converter ê°€ì ¸ì˜¤ê¸°
            if not hasattr(self, '_easyocr_reader'):
                import easyocr
                self._easyocr_reader = easyocr.Reader(['en'], gpu=False)
                print(f"[ğŸ” EasyOCR] Reader ì´ˆê¸°í™” ì™„ë£Œ - ë¬¸ìì…‹: {len(self._easyocr_reader.character)}ê°œ")
            
            # ì¶œë ¥ì„ PyTorch Tensorë¡œ ë³€í™˜
            if isinstance(output, np.ndarray):
                preds = torch.from_numpy(output).float()
            else:
                preds = output
            
            print(f"[ğŸ” CTC ë””ì½”ë”©] ì…ë ¥ í˜•íƒœ: {preds.shape}")
            
            # Softmax ì ìš©
            preds = F.softmax(preds, dim=2)
            
            # Greedy ë””ì½”ë”© (í…ŒìŠ¤íŠ¸ì—ì„œ ê²€ì¦ëœ ë°©ì‹)
            _, preds_index = preds.max(2)
            
            print(f"[ğŸ” CTC ë””ì½”ë”©] ì¸ë±ìŠ¤ í˜•íƒœ: {preds_index.shape}")
            
            # ê¸¸ì´ ì •ë³´ ìƒì„±
            batch_size = preds.size(0)
            preds_size = torch.IntTensor([preds.size(1)] * batch_size)
            
            # ì‹¤ì œ EasyOCR Converterë¡œ ë””ì½”ë”©
            converter = self._easyocr_reader.converter
            preds_str = converter.decode_greedy(
                preds_index.view(-1).data.cpu().detach().numpy(), 
                preds_size.data
            )
            
            decoded_text = preds_str[0] if preds_str else ""
            
            print(f"[âœ… CTC ë””ì½”ë”©] ê²°ê³¼: '{decoded_text}'")
            return decoded_text
            
        except Exception as e:
            print(f"[âš ï¸ EasyOCR Converter] ë””ì½”ë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return "???"
